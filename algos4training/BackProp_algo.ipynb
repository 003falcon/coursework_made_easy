{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3+n6qZ/x4DWALfVWXNjLR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/003falcon/coursework_made_easy/blob/main/algos4training/BackProp_algo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(precision=6, suppress=True)"
      ],
      "metadata": {
        "id": "PCRmn6sTA6TY"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation functions\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)"
      ],
      "metadata": {
        "id": "bdVsesSFBCH4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training"
      ],
      "metadata": {
        "id": "DzsXUwmrSYf-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NzfRl0zAwhT",
        "outputId": "60c925ce-34f2-41f4-82f6-cb24ed85e730"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "change in weights of hidden layer\n",
            "[[0.187885 0.247337]\n",
            " [0.000422 0.000556]\n",
            " [0.188271 0.247844]\n",
            " [0.003393 0.004467]\n",
            " [0.184525 0.242913]]\n",
            "Input layer updated weights\n",
            "[[ 0.200125 -0.900051  0.300137 -0.399983  0.099724]\n",
            " [-0.100624  0.200256 -0.700683  0.599915 -0.49862 ]\n",
            " [ 0.800749 -0.700307  0.40082  -0.099899  0.198344]]\n",
            "change in weights of hidden layer\n",
            "[[0.188412 0.232071]\n",
            " [0.000085 0.000105]\n",
            " [0.188466 0.232138]\n",
            " [0.001541 0.001898]\n",
            " [0.186814 0.230103]]\n",
            "Input layer updated weights\n",
            "[[ 0.200214 -0.900056  0.300205 -0.399923  0.100424]\n",
            " [-0.101158  0.200287 -0.701093  0.599554 -0.502821]\n",
            " [ 0.801461 -0.700349  0.401366 -0.099417  0.203945]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Inputs (a, b, c)\n",
        "X = np.array([\n",
        "    [1, -5, 6],   # Pattern 1\n",
        "    [1, -6, 8]    # Pattern 2\n",
        "])\n",
        "\n",
        "# Desired outputs (roots)\n",
        "y = np.array([\n",
        "    [2, 3],   # Roots of x^2 - 5x + 6 = 0\n",
        "    [2, 4]    # Roots of x^2 - 6x + 8 = 0\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "# Initialize weights/biases\n",
        "np.random.seed(42)\n",
        "input_size = X[0].shape[0]\n",
        "hidden_size = 5\n",
        "output_size = y[0].shape[0]\n",
        "\n",
        "wij=np.array(\n",
        "    [[0.2,-0.9,0.3,-0.4,0.1],\n",
        "    [-0.1,0.2,-0.7,0.6,-0.5],\n",
        "    [0.8,-0.7,0.4,-0.1,0.2]])\n",
        "\n",
        "\n",
        "wjk=np.array(\n",
        "    [[-0.1,0.2],\n",
        "    [0.8,-0.7],\n",
        "    [-0.3,0.5],\n",
        "    [0.4,-0.3],\n",
        "    [-0.2,0.1]])\n",
        "\n",
        "# Training loop\n",
        "epochs = 2\n",
        "lr = 0.5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    ### Forward pass\n",
        "    netj = wij.T.dot(X[epoch].T)\n",
        "    # print(netj.shape)\n",
        "\n",
        "    oj = sigmoid(netj)\n",
        "    # print(oj)\n",
        "\n",
        "    oj_deriv=sigmoid_derivative(oj)\n",
        "    # print(oj_deriv)\n",
        "\n",
        "    netk=wjk.T.dot(oj)\n",
        "    # print(netk)\n",
        "\n",
        "    ok=sigmoid(netk)\n",
        "    # print(ok)\n",
        "\n",
        "    ok_deriv=sigmoid_derivative(ok)\n",
        "    # print(ok_deriv)\n",
        "\n",
        "    output = ok  # linear output\n",
        "\n",
        "#     # Error\n",
        "    error = y[epoch] - output\n",
        "    # print(error.shape,oj.shape,ok_deriv.shape)\n",
        "\n",
        "\n",
        "    ### Backward pass\n",
        "    del_k = error*ok_deriv #just multiply element wise\n",
        "    del_wjk=np.outer(oj,del_k)\n",
        "    del_wjk*=lr\n",
        "    print(\"change in weights of hidden layer\")\n",
        "    print(del_wjk)\n",
        "\n",
        "    del_wij = np.matmul(wjk,del_k)\n",
        "    # print(del_wij)\n",
        "    del_wij*=oj_deriv*lr\n",
        "    del_wij=np.outer(X[epoch],del_wij)\n",
        "    # print(del_wij)\n",
        "\n",
        "    wij+=del_wij\n",
        "    wjk+=del_wjk\n",
        "    print(\"Input layer updated weights\")\n",
        "    print(wij)\n",
        "    # print(wjk)\n",
        "\n",
        "\n",
        "# print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing\n"
      ],
      "metadata": {
        "id": "m6BJuLfmSbiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test=[[1,-5.5,7]]\n",
        "for epoch in range(len(X_test)):\n",
        "### Forward pass\n",
        "    netj = wij.T.dot(X[epoch].T)\n",
        "    # print(netj.shape)\n",
        "\n",
        "    oj = sigmoid(netj)\n",
        "    # print(oj)\n",
        "\n",
        "    oj_deriv=sigmoid_derivative(oj)\n",
        "    # print(oj_deriv)\n",
        "\n",
        "    netk=wjk.T.dot(oj)\n",
        "    # print(netk)\n",
        "\n",
        "    ok=sigmoid(netk)\n",
        "    # print(ok)\n",
        "\n",
        "    ok_deriv=sigmoid_derivative(ok)\n",
        "    # print(ok_deriv)\n",
        "\n",
        "    output = ok  # linear output\n",
        "    print(ok)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvKgg9gWBRoN",
        "outputId": "791aa702-0a76-43c3-a7ab-b79826903792"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.62915861 0.90103471]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G_dMu-wXTL7y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}